{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변분법\n",
    "\n",
    " - 입력인 함수가 변할 떼 범함수의 출력이 어떻게 달라지는가를 계산하는 학문\n",
    " - 범함수\n",
    "     * function y(x) $\\rightarrow$ functional F $\\rightarrow$ real number\n",
    "     \n",
    " - 입력이 함수처럼 생겼는데 대괄호로 되어 있으면 범함수 : $F[y(x)]$\n",
    " \n",
    "      - $E[p(x)] = \\int^\\infty_{-\\infty} xp(x)dx$\n",
    "      \n",
    "      - $H[p(x)] = -\\int^\\infty_{-\\infty} p(x) log p(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 최적화 기초\n",
    "\n",
    " - 함수가 주어졌을 때 그 값이 가장 작아지거나 가장 커지는 위치를 찾는 것\n",
    " \n",
    "      - $x* = argmax_xf(x)$\n",
    "      - $x* = argmin_xf(x)$\n",
    "      \n",
    " #### 최소화 하려는 함수 $f(x)$ \n",
    "      - 목적함수 (objective function) \n",
    "      - 비용함수 (cost function)\n",
    "      - 손실함수 (loss function) \n",
    "      - 오차함수 (error function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### rosenbrock\n",
    "$\\rightarrow f(x,y) = (1-x)^2 + 100(y-x^2)^2$\n",
    "\n",
    "### grid search\n",
    "\n",
    " - x값을 넣어보고 그 중 가장 작은 값을 선택하는 방식\n",
    " - 높은 차원(4차 이상)에서는 사용하기 힘들다는 단점.\n",
    " \n",
    "### 수치적 최적화(numerical optimization)\n",
    "\n",
    " - 적은 횟수의 시도로 x의 위치를 찾는 방법\n",
    " \n",
    "  1) 현재 위치 $x_k$가 최적점인지 판단하는 알고리즘\n",
    "     \n",
    "      - 필요조건\n",
    "      \n",
    "       1) $\\nabla f = 0$ / $f' = 0$\n",
    "       \n",
    "       2) $g = 0$\n",
    "       \n",
    "      - 충분조건\n",
    "      \n",
    "       1) 2차 도함수의 부호까지 필요 \n",
    "         - $f'' > 0 $\n",
    "         - Hessian array PD $\\rightarrow$ 최적점 \n",
    "      \n",
    "  2) 어떤 위치 $x_k$를 시도한 뒤, 다음 번 시도할 위치 $x_{k+1}$을 찾는 알고리즘\n",
    "  \n",
    "      - 최대경사법(Steeppest Gradient Descent / SGD)\n",
    "      - $x_{k+1} = x_k - \\mu\\nabla f(x_k) = x_k - \\mu g(x_k)$\n",
    "      \n",
    " ----\n",
    " \n",
    " - 목적함수를 수식으로 알 수 있는 경우 : 2차 도함수 : Hessian array\n",
    " - 딥러닝처럼 목적함수가 수식으로 쓰여지지 않는 경우 : momentum\n",
    " \n",
    " ----\n",
    " \n",
    " #### 2차 도함수를 사용한 뉴턴 방법\n",
    " \n",
    "  $x_{n+1} = x_n - [H f(x_n)]^{-1}\\nabla f(x_n)$\n",
    "  \n",
    "   $\\rightarrow$ 사람이 미분을 직접해야하는 경우가 생김 \n",
    "   \n",
    " #### Auasi-Newton (준 뉴턴) 방법\n",
    " \n",
    "  - BFGS (Broyden - Fletcher - Goldfarb - Shanno)\n",
    "  - CG (Conjugated Gradient)\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy 최적화\n",
    "\n",
    "   - func: 목적함수\n",
    "   - x0: 초깃값 벡터\n",
    "   - jac: (옵션) 그레디언트 벡터를 출력하는 함수\n",
    "\n",
    "---\n",
    " #### minimize() 명령의 결과는 OptimizeResult 클래스 객체로 다음 속성을 가진다.\n",
    "\n",
    "   - x: 최적화 해\n",
    "   - success: 최적화에 성공하면 True 반환\n",
    "   - status: 종료 상태. 최적화에 성공하면 0 반환\n",
    "   - message: 메시지 문자열\n",
    "   - fun: x 위치에서의 함수의 값\n",
    "   - jac: x 위치에서의 자코비안(그레디언트) 벡터의 값\n",
    "   - hess_inv: x 위치에서의 헤시안 행렬의 역행렬의 값 (준 뉴턴방법)\n",
    "   - nfev: 목적함수 호출 횟수\n",
    "   - njev: 자코비안 계산 횟수\n",
    "   - nhev: 헤시안 계산 횟수\n",
    "   - nit: x 이동 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 2.0\n",
      " hess_inv: array([[0.5]])\n",
      "      jac: array([0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 9\n",
      "      nit: 2\n",
      "     njev: 3\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.99999999])\n"
     ]
    }
   ],
   "source": [
    "# success 값이 True == 최적화에 성공했다는 뜻\n",
    "# nfev의 값을 줄이기 위해서는 def로 도함수 값을 미리 지정해서 넣어줘야 함.\n",
    "\n",
    "\n",
    "def f1(x):\n",
    "    return (x-2) ** 2 + 2\n",
    "\n",
    "\n",
    "x0 = 0\n",
    "result = sp.optimize.minimize(f1, x0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 2.0\n",
      " hess_inv: array([[0.5]])\n",
      "      jac: array([0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 3\n",
      "      nit: 2\n",
      "     njev: 3\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([2.])\n"
     ]
    }
   ],
   "source": [
    "def f1p(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "\n",
    "result = sp.optimize.minimize(f1, x0, jac=f1p)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2차원 목적함수 재정의 (벡터 입력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: -20246.68622892564\n",
      " hess_inv: array([[ 5.62789387e-03, -1.59855363e+00],\n",
      "       [-1.59855363e+00,  4.54061320e+02]])\n",
      "      jac: array([-22798.86914062,    -81.07983398])\n",
      "  message: 'Maximum number of iterations has been exceeded.'\n",
      "     nfev: 2064\n",
      "      nit: 400\n",
      "     njev: 516\n",
      "   status: 1\n",
      "  success: False\n",
      "        x: array([ -142.35210213, 20263.71558382])\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    return (1-x[0]**2 + 100.0 * (x[1] - x[0]**2)**2)\n",
    "\n",
    "\n",
    "x0 = (-2, -2)\n",
    "result = sp.optimize.minimize(f2, x0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습문제 5.1.1\n",
    "\n",
    " - 초기점 변경\n",
    " - $\\nabla f(x)$를 구현하여 $jac$ 인수로 주는 방법으로 계산속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 1.0\n",
      " hess_inv: array([[1, 0],\n",
      "       [0, 1]])\n",
      "      jac: array([-1.49011612e-08,  1.49011612e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 4\n",
      "      nit: 0\n",
      "     njev: 1\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    return (1-x[0]**2 + 100.0 * (x[1] - x[0]**2)**2)\n",
    "\n",
    "\n",
    "x0 = (0, 0)\n",
    "result = sp.optimize.minimize(f2, x0, jac=None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 1.0441274516428312e-09\n",
      " hess_inv: array([[0.4999371 , 0.99981521],\n",
      "       [0.99981521, 2.00451929]])\n",
      "      jac: array([ 2.65674300e-08, -1.38057787e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 36\n",
      "      nit: 23\n",
      "     njev: 36\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "def f2g(x):\n",
    "    return np.array((2.0*(x[0]-1)-400.0 * x[0] * (x[1] - x[0]**2),\n",
    "                     200.0*(x[1] - x[0]**2)))\n",
    "\n",
    "\n",
    "result = sp.optimize.minimize(f2, x0, jac=f2g)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "sympy.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sympy.symbols('x y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 400 x \\left(- x^{2} + y\\right) + 2 x - 2$"
      ],
      "text/plain": [
       "        ⎛   2    ⎞          \n",
       "- 400⋅x⋅⎝- x  + y⎠ + 2⋅x - 2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = (1 - x)**2 + 100*(y - x ** 2)**2\n",
    "sympy.diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 200 x^{2} + 200 y$"
      ],
      "text/plain": [
       "       2        \n",
       "- 200⋅x  + 200⋅y"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.diff(f, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
